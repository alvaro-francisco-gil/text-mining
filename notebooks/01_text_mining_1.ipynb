{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "server_ip = os.getenv('SERVER_IP')\n",
    "port = os.getenv('PORT')\n",
    "\n",
    "MODEL_URL = f\"http://{server_ip}:{port}/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.2.0+cu121\n",
      "CUDA available: True\n",
      "CUDA device name: NVIDIA GeForce RTX 4070 SUPER\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No CUDA device detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import UDPOS\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train, validation, and test datasets\n",
    "train_iter, valid_iter, test_iter = UDPOS(root='.data', split=('train', 'valid', 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Al', '-', 'Zaman', ':', 'American', 'forces', 'killed', 'Shaikh', 'Abdullah', 'al', '-', 'Ani', ',', 'the', 'preacher', 'at', 'the', 'mosque', 'in', 'the', 'town', 'of', 'Qaim', ',', 'near', 'the', 'Syrian', 'border', '.'], ['PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'ADJ', 'NOUN', 'VERB', 'PROPN', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'PUNCT', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT'], ['NNP', 'HYPH', 'NNP', ':', 'JJ', 'NNS', 'VBD', 'NNP', 'NNP', 'NNP', 'HYPH', 'NNP', ',', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNP', ',', 'IN', 'DT', 'JJ', 'NN', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Look at the first few items in the training set\n",
    "for i, item in enumerate(train_iter):\n",
    "    print(item)\n",
    "    if i == 0:  # Print first 5 items\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at QCRI/bert-base-multilingual-cased-pos-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Using /root/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Failed to load CUDA kernels. Mra requires custom CUDA kernels. Please verify that compatible versions of PyTorch and CUDA Toolkit are installed: CUDA_HOME environment variable is not set. Please set it to your CUDA install root.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"QCRI/bert-base-multilingual-cased-pos-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "pipeline = TokenClassificationPipeline(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'DT', 'score': 0.9997243, 'index': 1, 'word': 'A', 'start': 0, 'end': 1}, {'entity': 'NN', 'score': 0.9997472, 'index': 2, 'word': 'test', 'start': 2, 'end': 6}, {'entity': 'NN', 'score': 0.99973196, 'index': 3, 'word': 'example', 'start': 7, 'end': 14}]\n"
     ]
    }
   ],
   "source": [
    "outputs = pipeline(\"A test example\")\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to map Penn Treebank tags to UDPOS tags\n",
    "penn_to_udpos = {\n",
    "    'O': 'X',\n",
    "    '``': 'PUNCT',\n",
    "    ',': 'PUNCT',\n",
    "    ':': 'PUNCT',\n",
    "    '.': 'PUNCT',\n",
    "    \"''\": 'PUNCT',\n",
    "    '$': 'SYM',\n",
    "    '#': 'SYM',\n",
    "    'CC': 'CCONJ',\n",
    "    'CD': 'NUM',\n",
    "    'DT': 'DET',\n",
    "    'EX': 'PRON',\n",
    "    'FW': 'X',\n",
    "    'IN': 'ADP',\n",
    "    'JJ': 'ADJ',\n",
    "    'JJR': 'ADJ',\n",
    "    'JJS': 'ADJ',\n",
    "    '-LRB-': 'PUNCT',\n",
    "    'LS': 'X',\n",
    "    'MD': 'AUX',\n",
    "    'NN': 'NOUN',\n",
    "    'NNP': 'PROPN',\n",
    "    'NNPS': 'PROPN',\n",
    "    'NNS': 'NOUN',\n",
    "    'PDT': 'DET',\n",
    "    'POS': 'PART',\n",
    "    'PRP': 'PRON',\n",
    "    'PRP$': 'PRON',\n",
    "    'RB': 'ADV',\n",
    "    'RBR': 'ADV',\n",
    "    'RBS': 'ADV',\n",
    "    'RP': 'PART',\n",
    "    '-RRB-': 'PUNCT',\n",
    "    'SYM': 'SYM',\n",
    "    'TO': 'PART',\n",
    "    'UH': 'INTJ',\n",
    "    'VB': 'VERB',\n",
    "    'VBD': 'VERB',\n",
    "    'VBG': 'VERB',\n",
    "    'VBN': 'VERB',\n",
    "    'VBP': 'VERB',\n",
    "    'VBZ': 'VERB',\n",
    "    'WDT': 'DET',\n",
    "    'WP': 'PRON',\n",
    "    'WP$': 'PRON',\n",
    "    'WRB' : 'ADV'\n",
    "}\n",
    "\n",
    "def convert_penn_to_udpos(predictions):\n",
    "    \"\"\"\n",
    "    Convert a list of predictions with Penn Treebank tags to UDPOS tags.\n",
    "    \n",
    "    :param predictions: List of dictionaries with an `entity` key for Penn Treebank tag.\n",
    "                        Example: [{'entity':'DT', ...}, ...]\n",
    "    \n",
    "    :return: List of dictionaries with `entity` key converted to UDPOS tag.\n",
    "             Example: [{'entity':'DET', ...}, ...]\n",
    "    \"\"\"\n",
    "    \n",
    "    for prediction in predictions:\n",
    "        penn_tag = prediction['entity']\n",
    "        # Map the Penn Treebank tag to UDPOS tag, defaulting to `X` if not found\n",
    "        prediction['entity'] = penn_to_udpos.get(penn_tag, \"X\")\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'DET', 'score': 0.9997243, 'index': 1, 'word': 'A', 'start': 0, 'end': 1}, {'entity': 'NOUN', 'score': 0.9997472, 'index': 2, 'word': 'test', 'start': 2, 'end': 6}, {'entity': 'NOUN', 'score': 0.99973196, 'index': 3, 'word': 'example', 'start': 7, 'end': 14}]\n"
     ]
    }
   ],
   "source": [
    "# Example model output\n",
    "example_output = [\n",
    "    {'entity': \"DT\",  \"score\": 0.9997243,  \"index\": 1, \"word\": \"A\", \"start\": 0, \"end\": 1},\n",
    "    {'entity': \"NN\",  \"score\": 0.9997472,  \"index\": 2, \"word\": \"test\", \"start\": 2, \"end\": 6},\n",
    "    {'entity': \"NN\",  \"score\": 0.99973196, \"index\": 3, \"word\": \"example\", \"start\": 7, \"end\": 14}\n",
    "]\n",
    "\n",
    "# Convert the Penn Treebank tags to UDPOS tags\n",
    "converted_output = convert_penn_to_udpos(example_output)\n",
    "\n",
    "# Print the converted output\n",
    "print(converted_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 0, Tag: O\n",
      "ID: 1, Tag: ``\n",
      "ID: 2, Tag: ,\n",
      "ID: 3, Tag: :\n",
      "ID: 4, Tag: .\n",
      "ID: 5, Tag: ''\n",
      "ID: 6, Tag: $\n",
      "ID: 7, Tag: #\n",
      "ID: 8, Tag: CC\n",
      "ID: 9, Tag: CD\n",
      "ID: 10, Tag: DT\n",
      "ID: 11, Tag: EX\n",
      "ID: 12, Tag: FW\n",
      "ID: 13, Tag: IN\n",
      "ID: 14, Tag: JJ\n",
      "ID: 15, Tag: JJR\n",
      "ID: 16, Tag: JJS\n",
      "ID: 17, Tag: -LRB-\n",
      "ID: 18, Tag: LS\n",
      "ID: 19, Tag: MD\n",
      "ID: 20, Tag: NN\n",
      "ID: 21, Tag: NNP\n",
      "ID: 22, Tag: NNPS\n",
      "ID: 23, Tag: NNS\n",
      "ID: 24, Tag: PDT\n",
      "ID: 25, Tag: POS\n",
      "ID: 26, Tag: PRP\n",
      "ID: 27, Tag: PRP$\n",
      "ID: 28, Tag: RB\n",
      "ID: 29, Tag: RBR\n",
      "ID: 30, Tag: RBS\n",
      "ID: 31, Tag: RP\n",
      "ID: 32, Tag: -RRB-\n",
      "ID: 33, Tag: SYM\n",
      "ID: 34, Tag: TO\n",
      "ID: 35, Tag: UH\n",
      "ID: 36, Tag: VB\n",
      "ID: 37, Tag: VBD\n",
      "ID: 38, Tag: VBG\n",
      "ID: 39, Tag: VBN\n",
      "ID: 40, Tag: VBP\n",
      "ID: 41, Tag: VBZ\n",
      "ID: 42, Tag: WDT\n",
      "ID: 43, Tag: WP\n",
      "ID: 44, Tag: WP$\n",
      "ID: 45, Tag: WRB\n"
     ]
    }
   ],
   "source": [
    "label_map = model.config.id2label\n",
    "\n",
    "for id, label in label_map.items():\n",
    "    print(f\"ID: {id}, Tag: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Step 1: List available models\n",
    "def get_available_models():\n",
    "    try:\n",
    "        response = requests.get(f\"{MODEL_URL}/models\")\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        models = response.json()\n",
    "        print(\"Available Models:\", json.dumps(models, indent=4))\n",
    "        return models.get(\"models\", [])\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching models: {e}\")\n",
    "        return []\n",
    "\n",
    "# Step 2: Generate a response using /v1/chat/completions\n",
    "def generate_chat_response(model_id, messages):\n",
    "    try:\n",
    "        # Prepare the payload for the chat completion request\n",
    "        payload = {\n",
    "            \"model\": model_id,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_tokens\": 100\n",
    "        }\n",
    "        \n",
    "        # Send POST request\n",
    "        response = requests.post(f\"{MODEL_URL}/chat/completions\", json=payload)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        \n",
    "        # Parse and display the response\n",
    "        result = response.json()\n",
    "        print(\"Chat Response:\", json.dumps(result, indent=4))\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating chat response: {e}\")\n",
    "        return None\n",
    "\n",
    "# Step 3: Generate a response using /v1/completions (optional)\n",
    "def generate_text_completion(model_id, prompt):\n",
    "    try:\n",
    "        # Prepare the payload for the text completion request\n",
    "        payload = {\n",
    "            \"model\": model_id,\n",
    "            \"prompt\": prompt,\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_tokens\": 100\n",
    "        }\n",
    "        \n",
    "        # Send POST request\n",
    "        response = requests.post(f\"{MODEL_URL}/completions\", json=payload)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        \n",
    "        # Parse and display the response\n",
    "        result = response.json()\n",
    "        print(\"Text Completion Response:\", json.dumps(result, indent=4))\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating text completion: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Models: {\n",
      "    \"data\": [\n",
      "        {\n",
      "            \"id\": \"mixtral-8x7b-instruct-v0.1\",\n",
      "            \"object\": \"model\",\n",
      "            \"owned_by\": \"organization_owner\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"text-embedding-nomic-embed-text-v1.5\",\n",
      "            \"object\": \"model\",\n",
      "            \"owned_by\": \"organization_owner\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"hermes-3-llama-3.1-8b\",\n",
      "            \"object\": \"model\",\n",
      "            \"owned_by\": \"organization_owner\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"llama-3.2-3b-instruct\",\n",
      "            \"object\": \"model\",\n",
      "            \"owned_by\": \"organization_owner\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"llama-3.2-1b-instruct\",\n",
      "            \"object\": \"model\",\n",
      "            \"owned_by\": \"organization_owner\"\n",
      "        }\n",
      "    ],\n",
      "    \"object\": \"list\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Models: {\n",
      "    \"data\": [\n",
      "        {\n",
      "            \"id\": \"mixtral-8x7b-instruct-v0.1\",\n",
      "            \"object\": \"model\",\n",
      "            \"owned_by\": \"organization_owner\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"text-embedding-nomic-embed-text-v1.5\",\n",
      "            \"object\": \"model\",\n",
      "            \"owned_by\": \"organization_owner\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"hermes-3-llama-3.1-8b\",\n",
      "            \"object\": \"model\",\n",
      "            \"owned_by\": \"organization_owner\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"llama-3.2-3b-instruct\",\n",
      "            \"object\": \"model\",\n",
      "            \"owned_by\": \"organization_owner\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"llama-3.2-1b-instruct\",\n",
      "            \"object\": \"model\",\n",
      "            \"owned_by\": \"organization_owner\"\n",
      "        }\n",
      "    ],\n",
      "    \"object\": \"list\"\n",
      "}\n",
      "No models available. Make sure LM Studio is running and a model is loaded.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Main function to demonstrate usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Get available models\n",
    "    models = get_available_models()\n",
    "    \n",
    "    if not models:\n",
    "        print(\"No models available. Make sure LM Studio is running and a model is loaded.\")\n",
    "    else:\n",
    "        # Select the first model (or replace with your desired model ID)\n",
    "        selected_model_id = models[0][\"mixtral-8x7b-instruct-v0.1\"]\n",
    "        \n",
    "        # Example for /v1/chat/completions\n",
    "        chat_messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Hello! How are you?\"}\n",
    "        ]\n",
    "        \n",
    "        print(\"\\n--- Chat Completion Example ---\")\n",
    "        generate_chat_response(selected_model_id, chat_messages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
