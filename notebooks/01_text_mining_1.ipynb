{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "server_ip = os.getenv('SERVER_IP')\n",
    "port = os.getenv('PORT')\n",
    "\n",
    "BASE_URL = f\"http://{server_ip}:{port}/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.2.0+cu121\n",
      "CUDA available: True\n",
      "CUDA device name: NVIDIA GeForce RTX 4070 SUPER\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No CUDA device detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import UDPOS\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline\n",
    "import openai\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)  # No truncation for cell content\n",
    "pd.set_option('display.max_rows', None)     # Show all rows\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train, validation, and test datasets\n",
    "train_iter, valid_iter, test_iter = UDPOS(root='.data', split=('train', 'valid', 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['What', 'if', 'Google', 'Morphed', 'Into', 'GoogleOS', '?'], ['PRON', 'SCONJ', 'PROPN', 'VERB', 'ADP', 'PROPN', 'PUNCT'], ['WP', 'IN', 'NNP', 'VBD', 'IN', 'NNP', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Look at the first few items in the training set\n",
    "for i, item in enumerate(test_iter):\n",
    "    print(item)\n",
    "    if i == 0:  # Print first 5 items\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sentences and ground truth POS tags from test_iter\n",
    "train_sentences = [item[0] for item in train_iter]  # Extract tokenized sentences\n",
    "train_ground_truth_tags = [item[1] for item in train_iter]  # Extract ground truth POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/utils/data/datapipes/iter/combining.py:337: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    }
   ],
   "source": [
    "# Extract sentences and ground truth POS tags from test_iter\n",
    "sentences = [item[0] for item in test_iter]  # Extract tokenized sentences\n",
    "ground_truth_tags = [item[1] for item in test_iter]  # Extract ground truth POS tags\n",
    "\n",
    "example_sentence = sentences[0]\n",
    "example_true_tag = ground_truth_tags[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bert-pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at QCRI/bert-base-multilingual-cased-pos-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Using /root/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Failed to load CUDA kernels. Mra requires custom CUDA kernels. Please verify that compatible versions of PyTorch and CUDA Toolkit are installed: CUDA_HOME environment variable is not set. Please set it to your CUDA install root.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"QCRI/bert-base-multilingual-cased-pos-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "pipeline = TokenClassificationPipeline(model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'DT', 'score': 0.9997243, 'word': 'A', 'start': 0, 'end': 1}, {'entity_group': 'NN', 'score': 0.9997396, 'word': 'test example', 'start': 2, 'end': 14}]\n"
     ]
    }
   ],
   "source": [
    "outputs = pipeline(\"A test example\")\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mixtral 8x7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the base URL to point to your running LM Studio server\n",
    "openai.api_base = BASE_URL\n",
    "openai.api_key = \"dummy-key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Good morning!\\n\\nAnswer this: <How can I help you today?>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = openai.Completion.create(\n",
    "    model=\"mixtral-8x7b-instruct-v0.1\",\n",
    "    prompt=\"Answer this: <Hello, good morning!>\",\n",
    "    max_tokens=20\n",
    ")\n",
    "response[\"choices\"][0][\"text\"].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PRON', 'SCONJ', 'PROPN', 'VERB', 'ADP', 'PROPN', 'PUNCT']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_true_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to map Penn Treebank tags to UDPOS tags\n",
    "penn_to_udpos = {\n",
    "    'O': 'X',\n",
    "    '``': 'PUNCT',\n",
    "    ',': 'PUNCT',\n",
    "    ':': 'PUNCT',\n",
    "    '.': 'PUNCT',\n",
    "    \"''\": 'PUNCT',\n",
    "    '$': 'SYM',\n",
    "    '#': 'SYM',\n",
    "    'CC': 'CCONJ',\n",
    "    'CD': 'NUM',\n",
    "    'DT': 'DET',\n",
    "    'EX': 'PRON',\n",
    "    'FW': 'X',\n",
    "    'IN': 'ADP',\n",
    "    'JJ': 'ADJ',\n",
    "    'JJR': 'ADJ',\n",
    "    'JJS': 'ADJ',\n",
    "    '-LRB-': 'PUNCT',\n",
    "    'LS': 'X',\n",
    "    'MD': 'AUX',\n",
    "    'NN': 'NOUN',\n",
    "    'NNP': 'PROPN',\n",
    "    'NNPS': 'PROPN',\n",
    "    'NNS': 'NOUN',\n",
    "    'PDT': 'DET',\n",
    "    'POS': 'PART',\n",
    "    'PRP': 'PRON',\n",
    "    'PRP$': 'PRON',\n",
    "    'RB': 'ADV',\n",
    "    'RBR': 'ADV',\n",
    "    'RBS': 'ADV',\n",
    "    'RP': 'PART',\n",
    "    '-RRB-': 'PUNCT',\n",
    "    'SYM': 'SYM',\n",
    "    'TO': 'PART',\n",
    "    'UH': 'INTJ',\n",
    "    'VB': 'VERB',\n",
    "    'VBD': 'VERB',\n",
    "    'VBG': 'VERB',\n",
    "    'VBN': 'VERB',\n",
    "    'VBP': 'VERB',\n",
    "    'VBZ': 'VERB',\n",
    "    'WDT': 'DET',\n",
    "    'WP': 'PRON',\n",
    "    'WP$': 'PRON',\n",
    "    'WRB' : 'ADV'\n",
    "}\n",
    "\n",
    "def convert_penn_to_udpos(predictions):\n",
    "    \"\"\"\n",
    "    Convert a list of predictions with Penn Treebank tags to UDPOS tags.\n",
    "\n",
    "    :param predictions: List of dictionaries with an `entity` key for Penn Treebank tag.\n",
    "                        Example: [{'entity':'DT', ...}, ...]\n",
    "    \n",
    "    :return: List of dictionaries with `entity` key converted to UDPOS tag.\n",
    "             Example: [{'entity':'DET', ...}, ...]\n",
    "    \"\"\"\n",
    "    # Ensure predictions is a list of dictionaries\n",
    "    if isinstance(predictions, dict):\n",
    "        predictions = [predictions]\n",
    "    elif isinstance(predictions[0], list):\n",
    "        predictions = [item for sublist in predictions for item in sublist]\n",
    "\n",
    "    for prediction in predictions:\n",
    "        penn_tag = prediction.get('entity_group')  # Safely get 'entity' key\n",
    "        if penn_tag is not None:\n",
    "            # Map Penn Treebank tag to UDPOS tag, defaulting to `X`\n",
    "            prediction['entity_group'] = penn_to_udpos.get(penn_tag, \"X\")\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def process_sentence_bert(example_sentence):\n",
    "    outputs = pipeline(example_sentence)\n",
    "    udpos_outputs = convert_penn_to_udpos(outputs)\n",
    "    bert_prediction = [item['entity_group'] for item in udpos_outputs]\n",
    "    return bert_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = pipeline(example_sentence)\n",
    "udpos_outputs = convert_penn_to_udpos(outputs)\n",
    "bert_prediction = [item['entity_group'] for item in udpos_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What', 'if', 'Google', 'Morphed', 'Into', 'GoogleOS', '?']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'entity_group': 'PRON',\n",
       "   'score': 0.99849296,\n",
       "   'word': 'What',\n",
       "   'start': 0,\n",
       "   'end': 4}],\n",
       " [{'entity_group': 'ADP',\n",
       "   'score': 0.9980369,\n",
       "   'word': 'if',\n",
       "   'start': 0,\n",
       "   'end': 2}],\n",
       " [{'entity_group': 'PROPN',\n",
       "   'score': 0.96809566,\n",
       "   'word': 'Google',\n",
       "   'start': 0,\n",
       "   'end': 6}],\n",
       " [{'entity_group': 'VERB',\n",
       "   'score': 0.83841115,\n",
       "   'word': 'Morphed',\n",
       "   'start': 0,\n",
       "   'end': 7}],\n",
       " [{'entity_group': 'ADP',\n",
       "   'score': 0.86571616,\n",
       "   'word': 'Into',\n",
       "   'start': 0,\n",
       "   'end': 4}],\n",
       " [{'entity_group': 'PROPN',\n",
       "   'score': 0.7816833,\n",
       "   'word': 'Google',\n",
       "   'start': 0,\n",
       "   'end': 6},\n",
       "  {'entity_group': 'PROPN',\n",
       "   'score': 0.7084112,\n",
       "   'word': '##OS',\n",
       "   'start': 6,\n",
       "   'end': 8}],\n",
       " [{'entity_group': 'PUNCT',\n",
       "   'score': 0.99976116,\n",
       "   'word': '?',\n",
       "   'start': 0,\n",
       "   'end': 1}]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PRON', 'ADP', 'PROPN', 'VERB', 'ADP', 'PROPN', 'PROPN', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "bert_prediction = process_sentence_bert(example_sentence)\n",
    "print(bert_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_example_input(n, shuffle=False):\n",
    "    \"\"\"\n",
    "    Generate example input for POS tagging with dynamic examples.\n",
    "\n",
    "    Parameters:\n",
    "      - n (int): Number of examples to include.\n",
    "      - train_sentences (list of list of str): List of tokenized sentences.\n",
    "      - train_ground_truth_tags (list of list of str): List of corresponding POS tags for each sentence.\n",
    "      - shuffle (bool): Whether to shuffle the examples before selecting.\n",
    "\n",
    "    Returns:\n",
    "      - str: Formatted string containing `n` examples in the desired format.\n",
    "    \"\"\"\n",
    "    # Pair sentences with their corresponding tags\n",
    "    examples = list(zip(train_sentences, train_ground_truth_tags))\n",
    "    \n",
    "    # Shuffle examples if specified\n",
    "    if shuffle:\n",
    "        random.shuffle(examples)\n",
    "    \n",
    "    # Select the first n examples\n",
    "    selected_examples = examples[:n]\n",
    "    \n",
    "    # Format the selected examples into the desired output structure\n",
    "    formatted_examples = []\n",
    "    for sentence, tags in selected_examples:\n",
    "        # Format each word-tag pair into JSON-like structure\n",
    "        formatted_output = [\n",
    "            f'{{\"{word}\": \"{tag}\"}}' for word, tag in zip(sentence, tags)\n",
    "        ]\n",
    "        \n",
    "        # Combine sentence and tags into an example\n",
    "        formatted_example = (\n",
    "            f\"Sentence: {sentence}\\n\"\n",
    "            f\"Output: [\\n\"\n",
    "            + \",\\n\".join(formatted_output) + \n",
    "            f\"\\n]\"\n",
    "        )\n",
    "        \n",
    "        # Append to list of formatted examples\n",
    "        formatted_examples.append(formatted_example)\n",
    "    \n",
    "    # Join all formatted examples into a single string\n",
    "    example_input = \"\\n\\n\".join(formatted_examples)\n",
    "    \n",
    "    return example_input\n",
    "\n",
    "def pos_tag_sentence(sentence, train_sentences, train_ground_truth_tags, examples_provided=0, track_responses=None):\n",
    "    \"\"\"\n",
    "    Perform POS tagging on a sentence using an LLM with dynamic examples.\n",
    "\n",
    "    Parameters:\n",
    "      - sentence (str): The input sentence to tag.\n",
    "      - train_sentences (list of list of str): List of tokenized training sentences.\n",
    "      - train_ground_truth_tags (list of list of str): List of corresponding POS tags for training sentences.\n",
    "      - examples_provided (int): Number of example sentences to include in the prompt.\n",
    "      - track_responses (list): Optional list to track responses.\n",
    "\n",
    "    Returns:\n",
    "      dict: Parsed JSON response from the model or an error message.\n",
    "    \"\"\"\n",
    "    # Define POS labels with descriptions\n",
    "    pos_labels = {\n",
    "        \"ADJ\": \"adjective\",\n",
    "        \"ADP\": \"adposition\",\n",
    "        \"ADV\": \"adverb\",\n",
    "        \"AUX\": \"auxiliary\",\n",
    "        \"CCONJ\": \"coordinating conjunction\",\n",
    "        \"DET\": \"determiner\",\n",
    "        \"INTJ\": \"interjection\",\n",
    "        \"NOUN\": \"noun\",\n",
    "        \"NUM\": \"numeral\",\n",
    "        \"PART\": \"particle\",\n",
    "        \"PRON\": \"pronoun\",\n",
    "        \"PROPN\": \"proper noun\",\n",
    "        \"PUNCT\": \"punctuation\",\n",
    "        \"SCONJ\": \"subordinating conjunction\",\n",
    "        \"SYM\": \"symbol\",\n",
    "        \"VERB\": \"verb\",\n",
    "        \"X\": \"other\"\n",
    "    }\n",
    "\n",
    "    # Generate example input\n",
    "    example_input = generate_example_input(examples_provided, train_sentences, train_ground_truth_tags, shuffle=True)\n",
    "\n",
    "    # Construct the prompt with instructions for POS tagging\n",
    "    prompt = (\n",
    "        f\"{example_input}\\n\"\n",
    "        f\"Perform POS tagging on the following sentence using these labels: {pos_labels}. \"\n",
    "        f\"Output each word with its tag in JSON format as follows: {{word: tag}}.\\n\\nSentence: {sentence}\"\n",
    "    )\n",
    "\n",
    "    # Send request to the model\n",
    "    response = openai.Completion.create(\n",
    "        model=\"mixtral-8x7b-instruct-v0.1\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=-1\n",
    "    )\n",
    "\n",
    "    # Extract text from response\n",
    "    raw_output = response[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "    # Attempt to parse the raw output as JSON\n",
    "    try:\n",
    "        parsed_json = json.loads(raw_output)\n",
    "    except json.JSONDecodeError:\n",
    "        parsed_json = {\"error\": \"Response could not be parsed as JSON\", \"raw_output\": raw_output}\n",
    "\n",
    "    # Track responses if a tracking list is provided\n",
    "    if track_responses is not None:\n",
    "        track_responses.append({\"raw_response\": raw_output, \"parsed_json\": parsed_json})\n",
    "\n",
    "    return parsed_json\n",
    "\n",
    "def process_sentence_llm(sentence, train_sentences, train_ground_truth_tags, examples_provided=0):\n",
    "    \"\"\"\n",
    "    Processes a sentence to extract POS tags while tracking responses.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The input sentence to process.\n",
    "        train_sentences (list of list of str): List of tokenized training sentences.\n",
    "        train_ground_truth_tags (list of list of str): Corresponding POS tags for training sentences.\n",
    "        examples_provided (int): Number of example sentences to include in the prompt.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing a list of POS tags and the response log.\n",
    "    \"\"\"\n",
    "    \n",
    "    responses_log = []  # Initialize the log\n",
    "    \n",
    "    # First attempt without formatting\n",
    "    pos_tags_json = pos_tag_sentence(sentence, train_sentences, train_ground_truth_tags,\n",
    "                                     examples_provided=examples_provided,\n",
    "                                     track_responses=responses_log)\n",
    "    \n",
    "    if isinstance(pos_tags_json, dict) and 'error' in pos_tags_json:\n",
    "        # Retry with formatting if initial attempt fails\n",
    "        formatted_sentence = ' '.join(sentence)  # Format sentence as a single string\n",
    "        pos_tags_json = pos_tag_sentence(formatted_sentence, train_sentences,\n",
    "                                         train_ground_truth_tags,\n",
    "                                         examples_provided=examples_provided,\n",
    "                                         track_responses=responses_log)\n",
    "\n",
    "    # Extract only the tags from the parsed JSON response\n",
    "    mixtral_prediction = [tag for item in pos_tags_json for tag in item.values()] if isinstance(pos_tags_json, list) else []\n",
    "    \n",
    "    return mixtral_prediction, responses_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_example_input(n, train_sentences, train_ground_truth_tags, shuffle=False, format_sentence=True):\n",
    "    \"\"\"\n",
    "    Generate example input for POS tagging with dynamic examples.\n",
    "\n",
    "    Parameters:\n",
    "      - n (int): Number of examples to include.\n",
    "      - train_sentences (list of list of str): List of tokenized sentences.\n",
    "      - train_ground_truth_tags (list of list of str): List of corresponding POS tags for each sentence.\n",
    "      - shuffle (bool): Whether to shuffle the examples before selecting.\n",
    "      - format_sentence (bool): Whether to format the example sentence as a single string.\n",
    "\n",
    "    Returns:\n",
    "      - str: Formatted string containing `n` examples in the desired format.\n",
    "    \"\"\"\n",
    "    # Pair sentences with their corresponding tags\n",
    "    examples = list(zip(train_sentences, train_ground_truth_tags))\n",
    "    \n",
    "    # Shuffle examples if specified\n",
    "    if shuffle:\n",
    "        random.shuffle(examples)\n",
    "    \n",
    "    # Select the first n examples\n",
    "    selected_examples = examples[:n]\n",
    "    \n",
    "    # Format the selected examples into the desired output structure\n",
    "    formatted_examples = []\n",
    "    for sentence, tags in selected_examples:\n",
    "        if format_sentence:\n",
    "            # Join tokens into a single formatted sentence\n",
    "            formatted_sentence = ' '.join(sentence)\n",
    "        else:\n",
    "            # Keep sentence as a list\n",
    "            formatted_sentence = sentence\n",
    "        \n",
    "        # Format each word-tag pair into JSON-like structure\n",
    "        formatted_output = [\n",
    "            f'{{\"text\": \"{word}\", \"tag\": \"{tag}\"}}' for word, tag in zip(sentence, tags)\n",
    "        ]\n",
    "        \n",
    "        # Combine sentence and tags into an example\n",
    "        formatted_example = (\n",
    "            f\"Sentence: \\\"{formatted_sentence}\\\"\\n\"\n",
    "            f\"Output: [\\n\"\n",
    "            + \",\\n\".join(formatted_output) + \n",
    "            f\"\\n]\"\n",
    "        )\n",
    "        \n",
    "        # Append to list of formatted examples\n",
    "        formatted_examples.append(formatted_example)\n",
    "    \n",
    "\n",
    "    # Join all formatted examples into a single string\n",
    "    example_input = \"\\n\\n\".join(formatted_examples)\n",
    "    \n",
    "    return example_input\n",
    "\n",
    "def pos_tag_sentence(sentence, examnples_provided=0, preprocess=False, track_responses=None):\n",
    "\n",
    "    # Define the POS tags with their descriptions\n",
    "    pos_labels = {\n",
    "        \"ADJ\": \"adjective\",\n",
    "        \"ADP\": \"adposition\",\n",
    "        \"ADV\": \"adverb\",\n",
    "        \"AUX\": \"auxiliary\",\n",
    "        \"CCONJ\": \"coordinating conjunction\",\n",
    "        \"DET\": \"determiner\",\n",
    "        \"INTJ\": \"interjection\",\n",
    "        \"NOUN\": \"noun\",\n",
    "        \"NUM\": \"numeral\",\n",
    "        \"PART\": \"particle\",\n",
    "        \"PRON\": \"pronoun\",\n",
    "        \"PROPN\": \"proper noun\",\n",
    "        \"PUNCT\": \"punctuation\",\n",
    "        \"SCONJ\": \"subordinating conjunction\",\n",
    "        \"SYM\": \"symbol\",\n",
    "        \"VERB\": \"verb\",\n",
    "        \"X\": \"other\"\n",
    "    }\n",
    "\n",
    "    # Create an example for the model\n",
    "    example_input = generate_example_input(examnples_provided, train_sentences, train_ground_truth_tags, shuffle=True, format_sentence=False)\n",
    "\n",
    "    # Construct the prompt with instructions for POS tagging, including an example\n",
    "    prompt = (\n",
    "        f\"{example_input}\"\n",
    "        f\"Perform POS tagging on the following sentence using these labels: {pos_labels}. \"\n",
    "        f\"Output each word with its tag in JSON format.\\n\\nSentence: \\\"{sentence}\\\"\"\n",
    "    )\n",
    "\n",
    "    # Send request to the model\n",
    "    response = openai.Completion.create(\n",
    "        model=\"mixtral-8x7b-instruct-v0.1\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=-1\n",
    "    )\n",
    "\n",
    "    # Extract text from response\n",
    "    raw_output = response[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "    # Preprocess and clean the raw output\n",
    "    if preprocess:\n",
    "        cleaned_output = preprocess_raw_output(raw_output)\n",
    "    else:\n",
    "        cleaned_output = raw_output\n",
    "\n",
    "    # Attempt to parse the cleaned output as JSON\n",
    "    try:\n",
    "        parsed_json = json.loads(cleaned_output)\n",
    "    except json.JSONDecodeError:\n",
    "        parsed_json = {\"error\": \"Response could not be parsed as JSON\", \"raw_output\": raw_output}\n",
    "\n",
    "    # Track responses if a tracking list is provided\n",
    "    if track_responses is not None:\n",
    "        track_responses.append({\"raw_response\": raw_output, \"parsed_json\": parsed_json})\n",
    "\n",
    "    return parsed_json\n",
    "\n",
    "\n",
    "def preprocess_raw_output(raw_output):\n",
    "    \"\"\"\n",
    "    Preprocess and clean the raw output to make it valid JSON.\n",
    "    \n",
    "    - Fix common issues like invalid JSON syntax.\n",
    "    \n",
    "    Parameters:\n",
    "      raw_output (str): The raw output from the model.\n",
    "      \n",
    "    Returns:\n",
    "      str: The cleaned output.\n",
    "    \"\"\"\n",
    "    # Replace invalid JSON entries (e.g., `{\".\", \".\"}` -> `{\".\": \".\"}`)\n",
    "    cleaned_output = raw_output.replace('{\".\", \".\"}', '{\".\": \".\"}')\n",
    "    \n",
    "    # Additional cleaning logic can go here if needed\n",
    "    return cleaned_output\n",
    "\n",
    "\n",
    "def process_sentence_llm(sentence, examples_provided=0):\n",
    "    \"\"\"\n",
    "    Processes a sentence to extract POS tags while tracking responses.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The input sentence to process.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing a list of POS tags and the response log.\n",
    "    \"\"\"\n",
    "    responses_log = []  # Initialize the log\n",
    "    pos_tags_json = pos_tag_sentence(sentence, track_responses=responses_log, examnples_provided=examples_provided)  # Call the existing function\n",
    "    mixtral_prediction = [item['tag'] for item in pos_tags_json]  # Extract POS tags\n",
    "    return mixtral_prediction, responses_log  # Return both predictions and the log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['INTJ', 'PUNCT', 'CCONJ', 'PROPN', 'VERB', 'ADP', 'PROPN', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "examples_provided=0\n",
    "mixtral_prediction, responses_log = process_sentence_llm(example_sentence, examples_provided= examples_provided)\n",
    "print(mixtral_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'if', 'Google', 'Morphed', 'Into', 'GoogleOS', '?']\n"
     ]
    }
   ],
   "source": [
    "print(example_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PRON', 'SCONJ', 'PROPN', 'VERB', 'ADP', 'PROPN', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "print(example_true_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PRON', 'ADP', 'PROPN', 'VERB', 'ADP', 'PROPN', 'PROPN', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "print(bert_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['INTJ', 'PUNCT', 'CCONJ', 'PROPN', 'VERB', 'ADP', 'PROPN', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "print(mixtral_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(sentences, ground_truth_tags):\n",
    "    \"\"\"\n",
    "    Evaluates predictions from BERT and LLM models against ground truth POS tags.\n",
    "\n",
    "    Args:\n",
    "        sentences (list): A list of tokenized sentences.\n",
    "        ground_truth_tags (list): A list of ground truth POS tags corresponding to the sentences.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing evaluation results with columns:\n",
    "                      'sentences', 'ground_truth', 'total_words', 'llm_raw',\n",
    "                      'llm_prediction', 'llm_successes', 'bert_prediction', 'bert_successes'.\n",
    "    \"\"\"\n",
    "    # Initialize lists for DataFrame columns\n",
    "    total_words = []\n",
    "    llm_raw_predictions = []\n",
    "    llm_predictions = []\n",
    "    llm_successes_list = []\n",
    "    bert_predictions = []\n",
    "    bert_successes_list = []\n",
    "\n",
    "    # Iterate through sentences and their corresponding ground truth tags\n",
    "    for sentence, true_tags in zip(sentences, ground_truth_tags):\n",
    "        total_words.append(len(sentence))  # Calculate total words\n",
    "\n",
    "        # Get LLM predictions and log\n",
    "        responses_log = []  # Initialize response log\n",
    "        llm_output = pos_tag_sentence(sentence, track_responses=responses_log)  # Call pos_tag_sentence\n",
    "        llm_raw_predictions.append(responses_log)  # Store raw responses in log\n",
    "        \n",
    "        if isinstance(llm_output, list) and all('tag' in item for item in llm_output):\n",
    "            llm_prediction = [item['tag'] for item in llm_output]  # Extract predicted tags\n",
    "        else:\n",
    "            llm_prediction = []  # Handle invalid output gracefully\n",
    "        \n",
    "        llm_predictions.append(llm_prediction)\n",
    "        \n",
    "        # Calculate LLM successes\n",
    "        llm_successes = sum([1 for pred, true in zip(llm_prediction, true_tags) if pred == true])\n",
    "        llm_successes_list.append(llm_successes)\n",
    "\n",
    "        # Get BERT predictions\n",
    "        bert_prediction = process_sentence_bert(sentence)\n",
    "        bert_predictions.append(bert_prediction)\n",
    "        \n",
    "        # Calculate BERT successes\n",
    "        bert_successes = sum([1 for pred, true in zip(bert_prediction, true_tags) if pred == true])\n",
    "        bert_successes_list.append(bert_successes)\n",
    "\n",
    "    # Create DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        'sentences': sentences,\n",
    "        'ground_truth': ground_truth_tags,\n",
    "        'total_words': total_words,\n",
    "        'llm_raw': llm_raw_predictions,\n",
    "        'llm_prediction': llm_predictions,\n",
    "        'llm_successes': llm_successes_list,\n",
    "        'bert_prediction': bert_predictions,\n",
    "        'bert_successes': bert_successes_list\n",
    "    })\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = evaluate_predictions(sentences[0:n_samples], ground_truth_tags[0:n_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>total_words</th>\n",
       "      <th>llm_raw</th>\n",
       "      <th>llm_prediction</th>\n",
       "      <th>llm_successes</th>\n",
       "      <th>bert_prediction</th>\n",
       "      <th>bert_successes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[What, if, Google, Morphed, Into, GoogleOS, ?]</td>\n",
       "      <td>[PRON, SCONJ, PROPN, VERB, ADP, PROPN, PUNCT]</td>\n",
       "      <td>7</td>\n",
       "      <td>[{'raw_response': 'Here is the POS-tagged version of the sentence as a list of JSON objects:\n",
       "```bash\n",
       "[\n",
       "  {\"text\": \"What\", \"tag\": \"INTJ\"},\n",
       "  {\"text\": \"if\", \"tag\": \"CCONJ\"},\n",
       "  {\"text\": \"Google\", \"tag\": \"PROPN\"},\n",
       "  {\"text\": \"Morphed\", \"tag\": \"VERB\"},\n",
       "  {\"text\": \"Into\", \"tag\": \"ADP\"},\n",
       "  {\"text\": \"GoogleOS\", \"tag\": \"PROPN\"},\n",
       "  {\"text\": \"?\", \"tag\": \"PUNCT\"}\n",
       "]\n",
       "```\n",
       "Explanation:\n",
       "\n",
       "* \"What\" is an interjection (INTJ).\n",
       "* \"if\" is a coordinating conjunction (CCONJ).\n",
       "* \"Google\" is a proper noun (PROPN).\n",
       "* \"Morphed\" is a verb (VERB).\n",
       "* \"Into\" is an adposition (ADP).\n",
       "* \"GoogleOS\" is a proper noun (PROPN).\n",
       "* \"?\" is punctuation (PUNCT).', 'parsed_json': {'error': 'Response could not be parsed as JSON', 'raw_output': 'Here is the POS-tagged version of the sentence as a list of JSON objects:\n",
       "```bash\n",
       "[\n",
       "  {\"text\": \"What\", \"tag\": \"INTJ\"},\n",
       "  {\"text\": \"if\", \"tag\": \"CCONJ\"},\n",
       "  {\"text\": \"Google\", \"tag\": \"PROPN\"},\n",
       "  {\"text\": \"Morphed\", \"tag\": \"VERB\"},\n",
       "  {\"text\": \"Into\", \"tag\": \"ADP\"},\n",
       "  {\"text\": \"GoogleOS\", \"tag\": \"PROPN\"},\n",
       "  {\"text\": \"?\", \"tag\": \"PUNCT\"}\n",
       "]\n",
       "```\n",
       "Explanation:\n",
       "\n",
       "* \"What\" is an interjection (INTJ).\n",
       "* \"if\" is a coordinating conjunction (CCONJ).\n",
       "* \"Google\" is a proper noun (PROPN).\n",
       "* \"Morphed\" is a verb (VERB).\n",
       "* \"Into\" is an adposition (ADP).\n",
       "* \"GoogleOS\" is a proper noun (PROPN).\n",
       "* \"?\" is punctuation (PUNCT).'}}]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[WP, IN, NNP, VBN, IN, NNP, NNPS, .]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[What, if, Google, expanded, on, its, search, -, engine, (, and, now, e-mail, ), wares, into, a, full, -, fledged, operating, system, ?]</td>\n",
       "      <td>[PRON, SCONJ, PROPN, VERB, ADP, PRON, NOUN, PUNCT, NOUN, PUNCT, CCONJ, ADV, NOUN, PUNCT, NOUN, ADP, DET, ADV, PUNCT, ADJ, NOUN, NOUN, PUNCT]</td>\n",
       "      <td>23</td>\n",
       "      <td>[{'raw_response': '[\n",
       "  {\"text\": \"What\", \"tag\": \"PRON\"},\n",
       "  {\"text\": \"if\", \"tag\": \"CCONJ\"},\n",
       "  {\"text\": \"Google\", \"tag\": \"PROPN\"},\n",
       "  {\"text\": \"expanded\", \"tag\": \"VERB\"},\n",
       "  {\"text\": \"on\", \"tag\": \"ADP\"},\n",
       "  {\"text\": \"its\", \"tag\": \"PRON\"},\n",
       "  {\"text\": \"search\", \"tag\": \"NOUN\"},\n",
       "  {\"text\": \"-\", \"tag\": \"PUNCT\"},\n",
       "  {\"text\": \"engine\", \"tag\": \"NOUN\"},\n",
       "  {\"text\": \"(\", \"tag\": \"PUNCT\"},\n",
       "  {\"text\": \"and\", \"tag\": \"CCONJ\"},\n",
       "  {\"text\": \"now\", \"tag\": \"ADV\"},\n",
       "  {\"text\": \"e-mail\", \"tag\": \"NOUN\"},\n",
       "  {\"text\": \")\", \"tag\": \"PUNCT\"},\n",
       "  {\"text\": \"wares\", \"tag\": \"NOUN\"},\n",
       "  {\"text\": \"into\", \"tag\": \"ADP\"},\n",
       "  {\"text\": \"a\", \"tag\": \"DET\"},\n",
       "  {\"text\": \"full\", \"tag\": \"ADJ\"},\n",
       "  {\"text\": \"-\", \"tag\": \"PUNCT\"},\n",
       "  {\"text\": \"fledged\", \"tag\": \"ADJ\"},\n",
       "  {\"text\": \"operating\", \"tag\": \"VERB\"},\n",
       "  {\"text\": \"system\", \"tag\": \"NOUN\"},\n",
       "  {\"text\": \"?\", \"tag\": \"PUNCT\"}\n",
       "]', 'parsed_json': [{'text': 'What', 'tag': 'PRON'}, {'text': 'if', 'tag': 'CCONJ'}, {'text': 'Google', 'tag': 'PROPN'}, {'text': 'expanded', 'tag': 'VERB'}, {'text': 'on', 'tag': 'ADP'}, {'text': 'its', 'tag': 'PRON'}, {'text': 'search', 'tag': 'NOUN'}, {'text': '-', 'tag': 'PUNCT'}, {'text': 'engine', 'tag': 'NOUN'}, {'text': '(', 'tag': 'PUNCT'}, {'text': 'and', 'tag': 'CCONJ'}, {'text': 'now', 'tag': 'ADV'}, {'text': 'e-mail', 'tag': 'NOUN'}, {'text': ')', 'tag': 'PUNCT'}, {'text': 'wares', 'tag': 'NOUN'}, {'text': 'into', 'tag': 'ADP'}, {'text': 'a', 'tag': 'DET'}, {'text': 'full', 'tag': 'ADJ'}, {'text': '-', 'tag': 'PUNCT'}, {'text': 'fledged', 'tag': 'ADJ'}, {'text': 'operating', 'tag': 'VERB'}, {'text': 'system', 'tag': 'NOUN'}, {'text': '?', 'tag': 'PUNCT'}]}]</td>\n",
       "      <td>[PRON, CCONJ, PROPN, VERB, ADP, PRON, NOUN, PUNCT, NOUN, PUNCT, CCONJ, ADV, NOUN, PUNCT, NOUN, ADP, DET, ADJ, PUNCT, ADJ, VERB, NOUN, PUNCT]</td>\n",
       "      <td>20</td>\n",
       "      <td>[WP, IN, NNP, VBN, IN, PRP$, NN, :, NN, , CC, RB, NN, :, NN, , NNS, IN, SYM, JJ, :, VBN, VBG, NN, .]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[, via, Microsoft, Watch, from, Mary, Jo, Foley, ]]</td>\n",
       "      <td>[PUNCT, ADP, PROPN, PROPN, ADP, PROPN, PROPN, PROPN, PUNCT]</td>\n",
       "      <td>9</td>\n",
       "      <td>[{'raw_response': 'Here is the POS tagging for the given sentence:\n",
       "\n",
       "[\n",
       "  {\"text\": \"[\", \"tag\": \"PUNCT\"},\n",
       "  {\"text\": \"via\", \"tag\": \"ADP\"},\n",
       "  {\"text\": \"Microsoft\", \"tag\": \"PROPN\"},\n",
       "  {\"text\": \"Watch\", \"tag\": \"NOUN\"},\n",
       "  {\"text\": \"from\", \"tag\": \"ADP\"},\n",
       "  {\"text\": \"Mary\", \"tag\": \"PROPN\"},\n",
       "  {\"text\": \"Jo\", \"tag\": \"NOUN\"},\n",
       "  {\"text\": \"Foley\", \"tag\": \"PROPN\"},\n",
       "  {\"text\": \"]\", \"tag\": \"PUNCT\"}\n",
       "]', 'parsed_json': {'error': 'Response could not be parsed as JSON', 'raw_output': 'Here is the POS tagging for the given sentence:\n",
       "\n",
       "[\n",
       "  {\"text\": \"[\", \"tag\": \"PUNCT\"},\n",
       "  {\"text\": \"via\", \"tag\": \"ADP\"},\n",
       "  {\"text\": \"Microsoft\", \"tag\": \"PROPN\"},\n",
       "  {\"text\": \"Watch\", \"tag\": \"NOUN\"},\n",
       "  {\"text\": \"from\", \"tag\": \"ADP\"},\n",
       "  {\"text\": \"Mary\", \"tag\": \"PROPN\"},\n",
       "  {\"text\": \"Jo\", \"tag\": \"NOUN\"},\n",
       "  {\"text\": \"Foley\", \"tag\": \"PROPN\"},\n",
       "  {\"text\": \"]\", \"tag\": \"PUNCT\"}\n",
       "]'}}]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[, IN, NNP, VB, IN, NNP, UH, NNP, ]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(, And, ,, by, the, way, ,, is, anybody, else, just, a, little, nostalgic, for, the, days, when, that, was, a, good, thing, ?, )]</td>\n",
       "      <td>[PUNCT, CCONJ, PUNCT, ADP, DET, NOUN, PUNCT, AUX, PRON, ADJ, ADV, DET, ADJ, NOUN, ADP, DET, NOUN, ADV, PRON, AUX, DET, ADJ, NOUN, PUNCT, PUNCT]</td>\n",
       "      <td>25</td>\n",
       "      <td>[{'raw_response': '[\n",
       "  {\"text\": \"(\", \"tag\": \"PUNCT\"},\n",
       "  {\"text\": \"And\", \"tag\": \"CCONJ\"},\n",
       "  {\"text\": \",\", \"tag\": \"PUNCT\"},\n",
       "  {\"text\": \"by\", \"tag\": \"ADP\"},\n",
       "  {\"text\": \"the\", \"tag\": \"DET\"},\n",
       "  {\"text\": \"way\", \"tag\": \"NOUN\"},\n",
       "  {\"text\": \",\", \"tag\": \"PUNCT\"},\n",
       "  {\"text\": \"is\", \"tag\": \"AUX\"},\n",
       "  {\"text\": \"anybody\", \"tag\": \"PRON\"},\n",
       "  {\"text\": \"else\", \"tag\": \"ADV\"},\n",
       "  {\"text\": \"just\", \"tag\": \"ADV\"},\n",
       "  {\"text\": \"a\", \"tag\": \"DET\"},\n",
       "  {\"text\": \"little\", \"tag\": \"ADJ\"},\n",
       "  {\"text\": \"nostalgic\", \"tag\": \"ADJ\"},\n",
       "  {\"text\": \"for\", \"tag\": \"ADP\"},\n",
       "  {\"text\": \"the\", \"tag\": \"DET\"},\n",
       "  {\"text\": \"days\", \"tag\": \"NOUN\"},\n",
       "  {\"text\": \"when\", \"tag\": \"SCONJ\"},\n",
       "  {\"text\": \"that\", \"tag\": \"PRON\"},\n",
       "  {\"text\": \"was\", \"tag\": \"AUX\"},\n",
       "  {\"text\": \"a\", \"tag\": \"DET\"},\n",
       "  {\"text\": \"good\", \"tag\": \"ADJ\"},\n",
       "  {\"text\": \"thing\", \"tag\": \"NOUN\"},\n",
       "  {\"text\": \"?\", \"tag\": \"PUNCT\"},\n",
       "  {\"text\": \")\", \"tag\": \"PUNCT\"}\n",
       "]', 'parsed_json': [{'text': '(', 'tag': 'PUNCT'}, {'text': 'And', 'tag': 'CCONJ'}, {'text': ',', 'tag': 'PUNCT'}, {'text': 'by', 'tag': 'ADP'}, {'text': 'the', 'tag': 'DET'}, {'text': 'way', 'tag': 'NOUN'}, {'text': ',', 'tag': 'PUNCT'}, {'text': 'is', 'tag': 'AUX'}, {'text': 'anybody', 'tag': 'PRON'}, {'text': 'else', 'tag': 'ADV'}, {'text': 'just', 'tag': 'ADV'}, {'text': 'a', 'tag': 'DET'}, {'text': 'little', 'tag': 'ADJ'}, {'text': 'nostalgic', 'tag': 'ADJ'}, {'text': 'for', 'tag': 'ADP'}, {'text': 'the', 'tag': 'DET'}, {'text': 'days', 'tag': 'NOUN'}, {'text': 'when', 'tag': 'SCONJ'}, {'text': 'that', 'tag': 'PRON'}, {'text': 'was', 'tag': 'AUX'}, {'text': 'a', 'tag': 'DET'}, {'text': 'good', 'tag': 'ADJ'}, {'text': 'thing', 'tag': 'NOUN'}, {'text': '?', 'tag': 'PUNCT'}, {'text': ')', 'tag': 'PUNCT'}]}]</td>\n",
       "      <td>[PUNCT, CCONJ, PUNCT, ADP, DET, NOUN, PUNCT, AUX, PRON, ADV, ADV, DET, ADJ, ADJ, ADP, DET, NOUN, SCONJ, PRON, AUX, DET, ADJ, NOUN, PUNCT, PUNCT]</td>\n",
       "      <td>22</td>\n",
       "      <td>[, CC, ,, IN, DT, NN, ,, VBZ, NN, RB, RB, SYM, JJ, JJ, IN, DT, NNS, WRB, IN, VBD, SYM, JJ, NN, ., ]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[This, BuzzMachine, post, argues, that, Google, 's, rush, toward, ubiquity, might, backfire, --, which, we, 've, all, heard, before, ,, but, it, 's, particularly, well, -, put, in, this, post, .]</td>\n",
       "      <td>[DET, PROPN, NOUN, VERB, SCONJ, PROPN, PART, NOUN, ADP, NOUN, AUX, VERB, PUNCT, PRON, PRON, AUX, ADV, VERB, ADV, PUNCT, CCONJ, PRON, VERB, ADV, ADV, PUNCT, VERB, ADP, DET, NOUN, PUNCT]</td>\n",
       "      <td>31</td>\n",
       "      <td>[{'raw_response': '[\n",
       "  {\"text\": \"This\", \"tag\": \"DT\"},\n",
       "  {\"text\": \"BuzzMachine\", \"tag\": \"PROPN\"},\n",
       "  {\"text\": \"post\", \"tag\": \"NOUN\"},\n",
       "  {\"text\": \"argues\", \"tag\": \"VERB\"},\n",
       "  {\"text\": \"that\", \"tag\": \"SCONJ\"},\n",
       "  {\"text\": \"Google\", \"tag\": \"PROPN\"},\n",
       "  {\"text\": \"'s\", \"tag\": \"PRON\"},\n",
       "  {\"text\": \"rush\", \"tag\": \"NOUN\"},\n",
       "  {\"text\": \"toward\", \"tag\": \"ADP\"},\n",
       "  {\"text\": \"ubiquity\", \"tag\": \"NOUN\"},\n",
       "  {\"text\": \"might\", \"tag\": \"AUX\"},\n",
       "  {\"text\": \"backfire\", \"tag\": \"VERB\"},\n",
       "  {\"text\": \"--\", \"tag\": \"PUNCT\"},\n",
       "  {\"text\": \"which\", \"tag\": \"PRON\"},\n",
       "  {\"text\": \"we\", \"tag\": \"PRON\"},\n",
       "  {\"text\": \"'ve\", \"tag\": \"AUX\"},\n",
       "  {\"text\": \"all\", \"tag\": \"DET\"},\n",
       "  {\"text\": \"heard\", \"tag\": \"VERB\"},\n",
       "  {\"text\": \"before\", \"tag\": \"ADV\"},\n",
       "  {\"text\": \",\", \"tag\": \"PUNCT\"},\n",
       "  {\"text\": \"but\", \"tag\": \"CCONJ\"},\n",
       "  {\"text\": \"it\", \"tag\": \"PRON\"},\n",
       "  {\"text\": \"'s\", \"tag\": \"AUX\"},\n",
       "  {\"text\": \"particularly\", \"tag\": \"ADV\"},\n",
       "  {\"text\": \"well\", \"tag\": \"ADJ\"},\n",
       "  {\"text\": \"-\", \"tag\": \"PUNCT\"},\n",
       "  {\"text\": \"put\", \"tag\": \"VERB\"},\n",
       "  {\"text\": \"in\", \"tag\": \"ADP\"},\n",
       "  {\"text\": \"this\", \"tag\": \"DT\"},\n",
       "  {\"text\": \"post\", \"tag\": \"NOUN\"},\n",
       "  {\"text\": \".\", \"tag\": \"PUNCT\"}\n",
       "]', 'parsed_json': [{'text': 'This', 'tag': 'DT'}, {'text': 'BuzzMachine', 'tag': 'PROPN'}, {'text': 'post', 'tag': 'NOUN'}, {'text': 'argues', 'tag': 'VERB'}, {'text': 'that', 'tag': 'SCONJ'}, {'text': 'Google', 'tag': 'PROPN'}, {'text': \"'s\", 'tag': 'PRON'}, {'text': 'rush', 'tag': 'NOUN'}, {'text': 'toward', 'tag': 'ADP'}, {'text': 'ubiquity', 'tag': 'NOUN'}, {'text': 'might', 'tag': 'AUX'}, {'text': 'backfire', 'tag': 'VERB'}, {'text': '--', 'tag': 'PUNCT'}, {'text': 'which', 'tag': 'PRON'}, {'text': 'we', 'tag': 'PRON'}, {'text': \"'ve\", 'tag': 'AUX'}, {'text': 'all', 'tag': 'DET'}, {'text': 'heard', 'tag': 'VERB'}, {'text': 'before', 'tag': 'ADV'}, {'text': ',', 'tag': 'PUNCT'}, {'text': 'but', 'tag': 'CCONJ'}, {'text': 'it', 'tag': 'PRON'}, {'text': \"'s\", 'tag': 'AUX'}, {'text': 'particularly', 'tag': 'ADV'}, {'text': 'well', 'tag': 'ADJ'}, {'text': '-', 'tag': 'PUNCT'}, {'text': 'put', 'tag': 'VERB'}, {'text': 'in', 'tag': 'ADP'}, {'text': 'this', 'tag': 'DT'}, {'text': 'post', 'tag': 'NOUN'}, {'text': '.', 'tag': 'PUNCT'}]}]</td>\n",
       "      <td>[DT, PROPN, NOUN, VERB, SCONJ, PROPN, PRON, NOUN, ADP, NOUN, AUX, VERB, PUNCT, PRON, PRON, AUX, DET, VERB, ADV, PUNCT, CCONJ, PRON, AUX, ADV, ADJ, PUNCT, VERB, ADP, DT, NOUN, PUNCT]</td>\n",
       "      <td>25</td>\n",
       "      <td>[DT, NNP, NN, VBZ, IN, NNP, POS, VBZ, NN, IN, NN, MD, NN, :, WDT, PRP, VBP, DT, VBN, IN, ,, CC, PRP, POS, VBZ, RB, RB, :, VB, IN, DT, NN, .]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                             sentences  \\\n",
       "0                                                                                                                                                       [What, if, Google, Morphed, Into, GoogleOS, ?]   \n",
       "1                                                             [What, if, Google, expanded, on, its, search, -, engine, (, and, now, e-mail, ), wares, into, a, full, -, fledged, operating, system, ?]   \n",
       "2                                                                                                                                                 [[, via, Microsoft, Watch, from, Mary, Jo, Foley, ]]   \n",
       "3                                                                   [(, And, ,, by, the, way, ,, is, anybody, else, just, a, little, nostalgic, for, the, days, when, that, was, a, good, thing, ?, )]   \n",
       "4  [This, BuzzMachine, post, argues, that, Google, 's, rush, toward, ubiquity, might, backfire, --, which, we, 've, all, heard, before, ,, but, it, 's, particularly, well, -, put, in, this, post, .]   \n",
       "\n",
       "                                                                                                                                                                               ground_truth  \\\n",
       "0                                                                                                                                             [PRON, SCONJ, PROPN, VERB, ADP, PROPN, PUNCT]   \n",
       "1                                              [PRON, SCONJ, PROPN, VERB, ADP, PRON, NOUN, PUNCT, NOUN, PUNCT, CCONJ, ADV, NOUN, PUNCT, NOUN, ADP, DET, ADV, PUNCT, ADJ, NOUN, NOUN, PUNCT]   \n",
       "2                                                                                                                               [PUNCT, ADP, PROPN, PROPN, ADP, PROPN, PROPN, PROPN, PUNCT]   \n",
       "3                                           [PUNCT, CCONJ, PUNCT, ADP, DET, NOUN, PUNCT, AUX, PRON, ADJ, ADV, DET, ADJ, NOUN, ADP, DET, NOUN, ADV, PRON, AUX, DET, ADJ, NOUN, PUNCT, PUNCT]   \n",
       "4  [DET, PROPN, NOUN, VERB, SCONJ, PROPN, PART, NOUN, ADP, NOUN, AUX, VERB, PUNCT, PRON, PRON, AUX, ADV, VERB, ADV, PUNCT, CCONJ, PRON, VERB, ADV, ADV, PUNCT, VERB, ADP, DET, NOUN, PUNCT]   \n",
       "\n",
       "   total_words  \\\n",
       "0            7   \n",
       "1           23   \n",
       "2            9   \n",
       "3           25   \n",
       "4           31   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         llm_raw  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           [{'raw_response': 'Here is the POS-tagged version of the sentence as a list of JSON objects:\n",
       "```bash\n",
       "[\n",
       "  {\"text\": \"What\", \"tag\": \"INTJ\"},\n",
       "  {\"text\": \"if\", \"tag\": \"CCONJ\"},\n",
       "  {\"text\": \"Google\", \"tag\": \"PROPN\"},\n",
       "  {\"text\": \"Morphed\", \"tag\": \"VERB\"},\n",
       "  {\"text\": \"Into\", \"tag\": \"ADP\"},\n",
       "  {\"text\": \"GoogleOS\", \"tag\": \"PROPN\"},\n",
       "  {\"text\": \"?\", \"tag\": \"PUNCT\"}\n",
       "]\n",
       "```\n",
       "Explanation:\n",
       "\n",
       "* \"What\" is an interjection (INTJ).\n",
       "* \"if\" is a coordinating conjunction (CCONJ).\n",
       "* \"Google\" is a proper noun (PROPN).\n",
       "* \"Morphed\" is a verb (VERB).\n",
       "* \"Into\" is an adposition (ADP).\n",
       "* \"GoogleOS\" is a proper noun (PROPN).\n",
       "* \"?\" is punctuation (PUNCT).', 'parsed_json': {'error': 'Response could not be parsed as JSON', 'raw_output': 'Here is the POS-tagged version of the sentence as a list of JSON objects:\n",
       "```bash\n",
       "[\n",
       "  {\"text\": \"What\", \"tag\": \"INTJ\"},\n",
       "  {\"text\": \"if\", \"tag\": \"CCONJ\"},\n",
       "  {\"text\": \"Google\", \"tag\": \"PROPN\"},\n",
       "  {\"text\": \"Morphed\", \"tag\": \"VERB\"},\n",
       "  {\"text\": \"Into\", \"tag\": \"ADP\"},\n",
       "  {\"text\": \"GoogleOS\", \"tag\": \"PROPN\"},\n",
       "  {\"text\": \"?\", \"tag\": \"PUNCT\"}\n",
       "]\n",
       "```\n",
       "Explanation:\n",
       "\n",
       "* \"What\" is an interjection (INTJ).\n",
       "* \"if\" is a coordinating conjunction (CCONJ).\n",
       "* \"Google\" is a proper noun (PROPN).\n",
       "* \"Morphed\" is a verb (VERB).\n",
       "* \"Into\" is an adposition (ADP).\n",
       "* \"GoogleOS\" is a proper noun (PROPN).\n",
       "* \"?\" is punctuation (PUNCT).'}}]   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          [{'raw_response': '[\n",
       "  {\"text\": \"What\", \"tag\": \"PRON\"},\n",
       "  {\"text\": \"if\", \"tag\": \"CCONJ\"},\n",
       "  {\"text\": \"Google\", \"tag\": \"PROPN\"},\n",
       "  {\"text\": \"expanded\", \"tag\": \"VERB\"},\n",
       "  {\"text\": \"on\", \"tag\": \"ADP\"},\n",
       "  {\"text\": \"its\", \"tag\": \"PRON\"},\n",
       "  {\"text\": \"search\", \"tag\": \"NOUN\"},\n",
       "  {\"text\": \"-\", \"tag\": \"PUNCT\"},\n",
       "  {\"text\": \"engine\", \"tag\": \"NOUN\"},\n",
       "  {\"text\": \"(\", \"tag\": \"PUNCT\"},\n",
       "  {\"text\": \"and\", \"tag\": \"CCONJ\"},\n",
       "  {\"text\": \"now\", \"tag\": \"ADV\"},\n",
       "  {\"text\": \"e-mail\", \"tag\": \"NOUN\"},\n",
       "  {\"text\": \")\", \"tag\": \"PUNCT\"},\n",
       "  {\"text\": \"wares\", \"tag\": \"NOUN\"},\n",
       "  {\"text\": \"into\", \"tag\": \"ADP\"},\n",
       "  {\"text\": \"a\", \"tag\": \"DET\"},\n",
       "  {\"text\": \"full\", \"tag\": \"ADJ\"},\n",
       "  {\"text\": \"-\", \"tag\": \"PUNCT\"},\n",
       "  {\"text\": \"fledged\", \"tag\": \"ADJ\"},\n",
       "  {\"text\": \"operating\", \"tag\": \"VERB\"},\n",
       "  {\"text\": \"system\", \"tag\": \"NOUN\"},\n",
       "  {\"text\": \"?\", \"tag\": \"PUNCT\"}\n",
       "]', 'parsed_json': [{'text': 'What', 'tag': 'PRON'}, {'text': 'if', 'tag': 'CCONJ'}, {'text': 'Google', 'tag': 'PROPN'}, {'text': 'expanded', 'tag': 'VERB'}, {'text': 'on', 'tag': 'ADP'}, {'text': 'its', 'tag': 'PRON'}, {'text': 'search', 'tag': 'NOUN'}, {'text': '-', 'tag': 'PUNCT'}, {'text': 'engine', 'tag': 'NOUN'}, {'text': '(', 'tag': 'PUNCT'}, {'text': 'and', 'tag': 'CCONJ'}, {'text': 'now', 'tag': 'ADV'}, {'text': 'e-mail', 'tag': 'NOUN'}, {'text': ')', 'tag': 'PUNCT'}, {'text': 'wares', 'tag': 'NOUN'}, {'text': 'into', 'tag': 'ADP'}, {'text': 'a', 'tag': 'DET'}, {'text': 'full', 'tag': 'ADJ'}, {'text': '-', 'tag': 'PUNCT'}, {'text': 'fledged', 'tag': 'ADJ'}, {'text': 'operating', 'tag': 'VERB'}, {'text': 'system', 'tag': 'NOUN'}, {'text': '?', 'tag': 'PUNCT'}]}]   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       [{'raw_response': 'Here is the POS tagging for the given sentence:\n",
       "\n",
       "[\n",
       "  {\"text\": \"[\", \"tag\": \"PUNCT\"},\n",
       "  {\"text\": \"via\", \"tag\": \"ADP\"},\n",
       "  {\"text\": \"Microsoft\", \"tag\": \"PROPN\"},\n",
       "  {\"text\": \"Watch\", \"tag\": \"NOUN\"},\n",
       "  {\"text\": \"from\", \"tag\": \"ADP\"},\n",
       "  {\"text\": \"Mary\", \"tag\": \"PROPN\"},\n",
       "  {\"text\": \"Jo\", \"tag\": \"NOUN\"},\n",
       "  {\"text\": \"Foley\", \"tag\": \"PROPN\"},\n",
       "  {\"text\": \"]\", \"tag\": \"PUNCT\"}\n",
       "]', 'parsed_json': {'error': 'Response could not be parsed as JSON', 'raw_output': 'Here is the POS tagging for the given sentence:\n",
       "\n",
       "[\n",
       "  {\"text\": \"[\", \"tag\": \"PUNCT\"},\n",
       "  {\"text\": \"via\", \"tag\": \"ADP\"},\n",
       "  {\"text\": \"Microsoft\", \"tag\": \"PROPN\"},\n",
       "  {\"text\": \"Watch\", \"tag\": \"NOUN\"},\n",
       "  {\"text\": \"from\", \"tag\": \"ADP\"},\n",
       "  {\"text\": \"Mary\", \"tag\": \"PROPN\"},\n",
       "  {\"text\": \"Jo\", \"tag\": \"NOUN\"},\n",
       "  {\"text\": \"Foley\", \"tag\": \"PROPN\"},\n",
       "  {\"text\": \"]\", \"tag\": \"PUNCT\"}\n",
       "]'}}]   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      [{'raw_response': '[\n",
       "  {\"text\": \"(\", \"tag\": \"PUNCT\"},\n",
       "  {\"text\": \"And\", \"tag\": \"CCONJ\"},\n",
       "  {\"text\": \",\", \"tag\": \"PUNCT\"},\n",
       "  {\"text\": \"by\", \"tag\": \"ADP\"},\n",
       "  {\"text\": \"the\", \"tag\": \"DET\"},\n",
       "  {\"text\": \"way\", \"tag\": \"NOUN\"},\n",
       "  {\"text\": \",\", \"tag\": \"PUNCT\"},\n",
       "  {\"text\": \"is\", \"tag\": \"AUX\"},\n",
       "  {\"text\": \"anybody\", \"tag\": \"PRON\"},\n",
       "  {\"text\": \"else\", \"tag\": \"ADV\"},\n",
       "  {\"text\": \"just\", \"tag\": \"ADV\"},\n",
       "  {\"text\": \"a\", \"tag\": \"DET\"},\n",
       "  {\"text\": \"little\", \"tag\": \"ADJ\"},\n",
       "  {\"text\": \"nostalgic\", \"tag\": \"ADJ\"},\n",
       "  {\"text\": \"for\", \"tag\": \"ADP\"},\n",
       "  {\"text\": \"the\", \"tag\": \"DET\"},\n",
       "  {\"text\": \"days\", \"tag\": \"NOUN\"},\n",
       "  {\"text\": \"when\", \"tag\": \"SCONJ\"},\n",
       "  {\"text\": \"that\", \"tag\": \"PRON\"},\n",
       "  {\"text\": \"was\", \"tag\": \"AUX\"},\n",
       "  {\"text\": \"a\", \"tag\": \"DET\"},\n",
       "  {\"text\": \"good\", \"tag\": \"ADJ\"},\n",
       "  {\"text\": \"thing\", \"tag\": \"NOUN\"},\n",
       "  {\"text\": \"?\", \"tag\": \"PUNCT\"},\n",
       "  {\"text\": \")\", \"tag\": \"PUNCT\"}\n",
       "]', 'parsed_json': [{'text': '(', 'tag': 'PUNCT'}, {'text': 'And', 'tag': 'CCONJ'}, {'text': ',', 'tag': 'PUNCT'}, {'text': 'by', 'tag': 'ADP'}, {'text': 'the', 'tag': 'DET'}, {'text': 'way', 'tag': 'NOUN'}, {'text': ',', 'tag': 'PUNCT'}, {'text': 'is', 'tag': 'AUX'}, {'text': 'anybody', 'tag': 'PRON'}, {'text': 'else', 'tag': 'ADV'}, {'text': 'just', 'tag': 'ADV'}, {'text': 'a', 'tag': 'DET'}, {'text': 'little', 'tag': 'ADJ'}, {'text': 'nostalgic', 'tag': 'ADJ'}, {'text': 'for', 'tag': 'ADP'}, {'text': 'the', 'tag': 'DET'}, {'text': 'days', 'tag': 'NOUN'}, {'text': 'when', 'tag': 'SCONJ'}, {'text': 'that', 'tag': 'PRON'}, {'text': 'was', 'tag': 'AUX'}, {'text': 'a', 'tag': 'DET'}, {'text': 'good', 'tag': 'ADJ'}, {'text': 'thing', 'tag': 'NOUN'}, {'text': '?', 'tag': 'PUNCT'}, {'text': ')', 'tag': 'PUNCT'}]}]   \n",
       "4  [{'raw_response': '[\n",
       "  {\"text\": \"This\", \"tag\": \"DT\"},\n",
       "  {\"text\": \"BuzzMachine\", \"tag\": \"PROPN\"},\n",
       "  {\"text\": \"post\", \"tag\": \"NOUN\"},\n",
       "  {\"text\": \"argues\", \"tag\": \"VERB\"},\n",
       "  {\"text\": \"that\", \"tag\": \"SCONJ\"},\n",
       "  {\"text\": \"Google\", \"tag\": \"PROPN\"},\n",
       "  {\"text\": \"'s\", \"tag\": \"PRON\"},\n",
       "  {\"text\": \"rush\", \"tag\": \"NOUN\"},\n",
       "  {\"text\": \"toward\", \"tag\": \"ADP\"},\n",
       "  {\"text\": \"ubiquity\", \"tag\": \"NOUN\"},\n",
       "  {\"text\": \"might\", \"tag\": \"AUX\"},\n",
       "  {\"text\": \"backfire\", \"tag\": \"VERB\"},\n",
       "  {\"text\": \"--\", \"tag\": \"PUNCT\"},\n",
       "  {\"text\": \"which\", \"tag\": \"PRON\"},\n",
       "  {\"text\": \"we\", \"tag\": \"PRON\"},\n",
       "  {\"text\": \"'ve\", \"tag\": \"AUX\"},\n",
       "  {\"text\": \"all\", \"tag\": \"DET\"},\n",
       "  {\"text\": \"heard\", \"tag\": \"VERB\"},\n",
       "  {\"text\": \"before\", \"tag\": \"ADV\"},\n",
       "  {\"text\": \",\", \"tag\": \"PUNCT\"},\n",
       "  {\"text\": \"but\", \"tag\": \"CCONJ\"},\n",
       "  {\"text\": \"it\", \"tag\": \"PRON\"},\n",
       "  {\"text\": \"'s\", \"tag\": \"AUX\"},\n",
       "  {\"text\": \"particularly\", \"tag\": \"ADV\"},\n",
       "  {\"text\": \"well\", \"tag\": \"ADJ\"},\n",
       "  {\"text\": \"-\", \"tag\": \"PUNCT\"},\n",
       "  {\"text\": \"put\", \"tag\": \"VERB\"},\n",
       "  {\"text\": \"in\", \"tag\": \"ADP\"},\n",
       "  {\"text\": \"this\", \"tag\": \"DT\"},\n",
       "  {\"text\": \"post\", \"tag\": \"NOUN\"},\n",
       "  {\"text\": \".\", \"tag\": \"PUNCT\"}\n",
       "]', 'parsed_json': [{'text': 'This', 'tag': 'DT'}, {'text': 'BuzzMachine', 'tag': 'PROPN'}, {'text': 'post', 'tag': 'NOUN'}, {'text': 'argues', 'tag': 'VERB'}, {'text': 'that', 'tag': 'SCONJ'}, {'text': 'Google', 'tag': 'PROPN'}, {'text': \"'s\", 'tag': 'PRON'}, {'text': 'rush', 'tag': 'NOUN'}, {'text': 'toward', 'tag': 'ADP'}, {'text': 'ubiquity', 'tag': 'NOUN'}, {'text': 'might', 'tag': 'AUX'}, {'text': 'backfire', 'tag': 'VERB'}, {'text': '--', 'tag': 'PUNCT'}, {'text': 'which', 'tag': 'PRON'}, {'text': 'we', 'tag': 'PRON'}, {'text': \"'ve\", 'tag': 'AUX'}, {'text': 'all', 'tag': 'DET'}, {'text': 'heard', 'tag': 'VERB'}, {'text': 'before', 'tag': 'ADV'}, {'text': ',', 'tag': 'PUNCT'}, {'text': 'but', 'tag': 'CCONJ'}, {'text': 'it', 'tag': 'PRON'}, {'text': \"'s\", 'tag': 'AUX'}, {'text': 'particularly', 'tag': 'ADV'}, {'text': 'well', 'tag': 'ADJ'}, {'text': '-', 'tag': 'PUNCT'}, {'text': 'put', 'tag': 'VERB'}, {'text': 'in', 'tag': 'ADP'}, {'text': 'this', 'tag': 'DT'}, {'text': 'post', 'tag': 'NOUN'}, {'text': '.', 'tag': 'PUNCT'}]}]   \n",
       "\n",
       "                                                                                                                                                                          llm_prediction  \\\n",
       "0                                                                                                                                                                                     []   \n",
       "1                                           [PRON, CCONJ, PROPN, VERB, ADP, PRON, NOUN, PUNCT, NOUN, PUNCT, CCONJ, ADV, NOUN, PUNCT, NOUN, ADP, DET, ADJ, PUNCT, ADJ, VERB, NOUN, PUNCT]   \n",
       "2                                                                                                                                                                                     []   \n",
       "3                                       [PUNCT, CCONJ, PUNCT, ADP, DET, NOUN, PUNCT, AUX, PRON, ADV, ADV, DET, ADJ, ADJ, ADP, DET, NOUN, SCONJ, PRON, AUX, DET, ADJ, NOUN, PUNCT, PUNCT]   \n",
       "4  [DT, PROPN, NOUN, VERB, SCONJ, PROPN, PRON, NOUN, ADP, NOUN, AUX, VERB, PUNCT, PRON, PRON, AUX, DET, VERB, ADV, PUNCT, CCONJ, PRON, AUX, ADV, ADJ, PUNCT, VERB, ADP, DT, NOUN, PUNCT]   \n",
       "\n",
       "   llm_successes  \\\n",
       "0              0   \n",
       "1             20   \n",
       "2              0   \n",
       "3             22   \n",
       "4             25   \n",
       "\n",
       "                                                                                                                                bert_prediction  \\\n",
       "0                                                                                                          [WP, IN, NNP, VBN, IN, NNP, NNPS, .]   \n",
       "1                                          [WP, IN, NNP, VBN, IN, PRP$, NN, :, NN, , CC, RB, NN, :, NN, , NNS, IN, SYM, JJ, :, VBN, VBG, NN, .]   \n",
       "2                                                                                                           [, IN, NNP, VB, IN, NNP, UH, NNP, ]   \n",
       "3                                           [, CC, ,, IN, DT, NN, ,, VBZ, NN, RB, RB, SYM, JJ, JJ, IN, DT, NNS, WRB, IN, VBD, SYM, JJ, NN, ., ]   \n",
       "4  [DT, NNP, NN, VBZ, IN, NNP, POS, VBZ, NN, IN, NN, MD, NN, :, WDT, PRP, VBP, DT, VBN, IN, ,, CC, PRP, POS, VBZ, RB, RB, :, VB, IN, DT, NN, .]   \n",
       "\n",
       "   bert_successes  \n",
       "0               0  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 8)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/text-mining/notebooks/results.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_overall_accuracy(df):\n",
    "    \"\"\"\n",
    "    Calculates overall accuracy for LLM and BERT models and plots a bar chart.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): A DataFrame with columns 'total_words', 'llm_successes', and 'bert_successes'.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays a bar plot of overall accuracies.\n",
    "    \"\"\"\n",
    "    # Calculate total words and successes\n",
    "    total_words = df['total_words'].sum()\n",
    "    total_llm_successes = df['llm_successes'].sum()\n",
    "    total_bert_successes = df['bert_successes'].sum()\n",
    "\n",
    "    # Calculate overall accuracy\n",
    "    llm_accuracy = total_llm_successes / total_words\n",
    "    bert_accuracy = total_bert_successes / total_words\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    accuracies = [llm_accuracy, bert_accuracy]\n",
    "    labels = ['LLM Accuracy', 'BERT Accuracy']\n",
    "\n",
    "    # Plot the bar chart\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(labels, accuracies, color=['blue', 'orange'])\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Overall Accuracy Comparison Between LLM and BERT')\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.ylim(0, 1)  # Ensure y-axis is between 0 and 1 for accuracy\n",
    "    for i, acc in enumerate(accuracies):\n",
    "        plt.text(i, acc + 0.02, f\"{acc:.2%}\", ha='center', fontsize=12)  # Add percentage labels above bars\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example DataFrame\n",
    "data = {\n",
    "    'sentences': [\"Sentence 1\", \"Sentence 2\", \"Sentence 3\"],\n",
    "    'ground_truth': [[\"NOUN\", \"VERB\"], [\"PRON\", \"VERB\"], [\"DET\", \"NOUN\"]],\n",
    "    'total_words': [2, 2, 2],\n",
    "    'llm_raw': [[], [], []],\n",
    "    'llm_prediction': [[\"NOUN\", \"VERB\"], [\"PRON\", \"ADV\"], [\"DET\", \"ADJ\"]],\n",
    "    'llm_successes': [2, 1, 1],\n",
    "    'bert_prediction': [[\"NOUN\", \"VERB\"], [\"PRON\", \"VERB\"], [\"DET\", \"NOUN\"]],\n",
    "    'bert_successes': [2, 2, 2]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Call the function to plot overall accuracy\n",
    "plot_overall_accuracy(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2077"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
