{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import gensim.downloader as api\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder_path = '/text-mining/data/02_text_representation/Corpus-representacion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_files_to_dict(folder_path):\n",
    "    files_dict = {}\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n",
    "                    files_dict[file_path] = f.read()\n",
    "            except Exception as e:\n",
    "                print(f\"Could not read file {file_path}: {e}\")\n",
    "    return files_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "805"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict = save_files_to_dict(data_folder_path)\n",
    "data = list(data_dict.values())\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xref: cantaloupe.srv.cs.cmu.edu rec.autos:102656 rec.autos.tech:53911 rec.autos.driving:16432 sci.electronics:53562\n",
      "Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv.cs.cmu.edu!fs7.ece.cmu.edu!europa.eng.gtefsd.com!gatech!darwin.sura.net!mojo.eng.umd.edu!russotto\n",
      "From: russotto@eng.umd.edu (Matthew T. Russotto)\n",
      "Newsgroups: rec.autos,rec.autos.tech,rec.autos.driving,sci.electronics\n",
      "Subject: Re: electronic odometers (was: Used BMW Question ..... ???)\n",
      "Date: 16 Apr 1993 03:51:56 GMT\n",
      "Organization: Project GLUE, University of Maryland, College Park\n",
      "Lines: 33\n",
      "Message-ID: <1qlagsINNka0@mojo.eng.umd.edu>\n",
      "References: <1qflgu$mpb@hpscit.sc.hp.com> <1993Apr14.153740.18542@nimbus.com> <1993Apr14.174857.28314@porthos.cc.bellcore.com>\n",
      "NNTP-Posting-Host: tea.eng.umd.edu\n",
      "\n",
      "In article <1993Apr14.174857.28314@porthos.cc.bellcore.com> dje@bmw535.NoSubdomain.NoDomain (Don Eilenberger) writes:\n",
      "}In article <1993Apr14.153740.18542@nimbus.com>, jimiii@nimbus.com (Jim Warford) writes:\n",
      "\n",
      "}|>  There are two simple procedures for alterating any odometer.\n",
      "}|> \n",
      "}|> 1. Mechanical driven odometer:\n",
      "}|>     Remove the speedo cable from the transmission.\n",
      "}|>     Attach a drill and run at max speed until the speedo turns over.\n",
      "}|>     Continue until the desired mileage is reached.\n",
      "}|> \n",
      "}|> 2. Electronically driven odometer:\n",
      "}|>     Remove the sensor wire from the sensor.\n",
      "}|>     Attach the Calibration out signal from an Oscope to the wire.\n",
      "}|>     Run until the speedo turns over and attains the desired mileage.\n",
      "}\n",
      "}Dear Faster.. I kinda wonder.. have you ever tried version 2? On what?\n",
      "}Since the sensor wire on a BMW feeds also into the computer.. and we\n",
      "}don't know what signal voltage is expected from it.. bad things\n",
      "}*could* happen... also since we don't know the pulse rate, we\n",
      "}may damage the analog part of the speedo (yes.. BMW uses a combined\n",
      "}instrument.. speed in analog, trip and total milage is digital) with\n",
      "}the needle pegged up against the 160MPH stop..\n",
      "}\n",
      "}Just a thought...\n",
      "\n",
      "You've got the oscilliscope, so you connect it up to the sensor wire\n",
      "and measure this stuff.  That way you know what it expects.\n",
      "\n",
      "-- \n",
      "Matthew T. Russotto\trussotto@eng.umd.edu\trussotto@wam.umd.edu\n",
      "Some news readers expect \"Disclaimer:\" here.\n",
      "Just say NO to police searches and seizures.  Make them use force.\n",
      "(not responsible for bodily harm resulting from following above advice)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_index = 527\n",
    "print(data[example_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Message Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_message_body(email_text):\n",
    "    \"\"\"\n",
    "    Extracts the main message body from an email by removing headers, unnecessary metadata, \n",
    "    and signatures, while preserving quoted lines that provide meaningful context.\n",
    "    \"\"\"\n",
    "    # Split the email into lines\n",
    "    lines = email_text.splitlines()\n",
    "\n",
    "    # Remove header (everything before the first blank line)\n",
    "    blank_line_index = next((i for i, line in enumerate(lines) if line.strip() == \"\"), None)\n",
    "    if blank_line_index is not None:\n",
    "        lines = lines[blank_line_index + 1:]\n",
    "\n",
    "    # Remove lines containing email addresses\n",
    "    email_pattern = re.compile(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\")\n",
    "    lines = [line for line in lines if not email_pattern.search(line)]\n",
    "\n",
    "    # Remove lines with only one to three capitalized words\n",
    "    capitalized_words_pattern = re.compile(r\"^([A-Z][a-z]+\\s?){1,3}$\")\n",
    "    lines = [line for line in lines if not capitalized_words_pattern.match(line.strip())]\n",
    "\n",
    "    # Remove lines after signature patterns\n",
    "    signature_patterns = [\n",
    "        r\"^--\\s*$\",  # Standard signature delimiter\n",
    "        r\"^>--\",     # Quoted signature delimiter\n",
    "        r\"^Kind regards\",  # Common closing phrases\n",
    "        r\"^Best regards\",\n",
    "        r\"^Sincerely\",\n",
    "        r\"^Sent from my iPhone\",\n",
    "        r\"^Sent from my BlackBerry\",\n",
    "        r\"^Confidentiality Notice\",  # Legal disclaimers\n",
    "    ]\n",
    "\n",
    "    filtered_lines = []\n",
    "    skip_lines = False\n",
    "\n",
    "    for line in lines:\n",
    "        # Check for signature patterns\n",
    "        if any(re.match(pattern, line.strip(), re.IGNORECASE) for pattern in signature_patterns):\n",
    "            skip_lines = True\n",
    "        # Stop skipping after a blank line\n",
    "        if skip_lines and line.strip() == \"\":\n",
    "            skip_lines = False\n",
    "            continue\n",
    "        # Skip lines if in the skip mode\n",
    "        if skip_lines:\n",
    "            continue\n",
    "\n",
    "        filtered_lines.append(line)\n",
    "\n",
    "    # Retain quoted lines unless they are irrelevant or part of a signature\n",
    "    meaningful_quoted_lines = []\n",
    "    for line in filtered_lines:\n",
    "        if line.strip().startswith(\">\"):\n",
    "            # Keep the line if itâ€™s not part of a quoted signature or irrelevant\n",
    "            if not re.match(r\"^>--\", line.strip()):\n",
    "                meaningful_quoted_lines.append(line)\n",
    "        else:\n",
    "            meaningful_quoted_lines.append(line)\n",
    "\n",
    "    # Join remaining lines to form the message body\n",
    "    message_body = \"\\n\".join(meaningful_quoted_lines).strip()\n",
    "    return message_body\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "}|>  There are two simple procedures for alterating any odometer.\n",
      "}|> \n",
      "}|> 1. Mechanical driven odometer:\n",
      "}|>     Remove the speedo cable from the transmission.\n",
      "}|>     Attach a drill and run at max speed until the speedo turns over.\n",
      "}|>     Continue until the desired mileage is reached.\n",
      "}|> \n",
      "}|> 2. Electronically driven odometer:\n",
      "}|>     Remove the sensor wire from the sensor.\n",
      "}|>     Attach the Calibration out signal from an Oscope to the wire.\n",
      "}|>     Run until the speedo turns over and attains the desired mileage.\n",
      "}\n",
      "}Dear Faster.. I kinda wonder.. have you ever tried version 2? On what?\n",
      "}Since the sensor wire on a BMW feeds also into the computer.. and we\n",
      "}don't know what signal voltage is expected from it.. bad things\n",
      "}*could* happen... also since we don't know the pulse rate, we\n",
      "}may damage the analog part of the speedo (yes.. BMW uses a combined\n",
      "}instrument.. speed in analog, trip and total milage is digital) with\n",
      "}the needle pegged up against the 160MPH stop..\n",
      "}\n",
      "}Just a thought...\n",
      "\n",
      "You've got the oscilliscope, so you connect it up to the sensor wire\n",
      "and measure this stuff.  That way you know what it expects.\n"
     ]
    }
   ],
   "source": [
    "data = [extract_message_body(email) for email in data]\n",
    "print(data[example_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, remove_numbers=True, use_lemmatization=True):\n",
    "    \"\"\"\n",
    "    Preprocesses text by removing punctuation, numbers, stop-words,\n",
    "    and applying lemmatization or stemming.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "        remove_numbers (bool): Whether to remove numbers from the text.\n",
    "        use_lemmatization (bool): If True, lemmatize; if False, apply stemming.\n",
    "\n",
    "    Returns:\n",
    "        str: The preprocessed text.\n",
    "    \"\"\"\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    \n",
    "    # Remove numbers if required\n",
    "    if remove_numbers:\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove stop-words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Apply lemmatization or stemming\n",
    "    if use_lemmatization:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    else:\n",
    "        stemmer = PorterStemmer()\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    # Join words back into a single string\n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two simple procedure alterating odometer mechanical driven odometer remove speedo cable transmission attach drill run max speed speedo turn continue desired mileage reached electronically driven odometer remove sensor wire sensor attach calibration signal oscope wire run speedo turn attains desired mileage dear faster kinda wonder ever tried version since sensor wire bmw feed also computer dont know signal voltage expected bad thing could happen also since dont know pulse rate may damage analog part speedo yes bmw us combined instrument speed analog trip total milage digital needle pegged mph stop thought youve got oscilliscope connect sensor wire measure stuff way know expects\n"
     ]
    }
   ],
   "source": [
    "data = [preprocess_text(email) for email in data]\n",
    "print(data[example_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_to_vectors(doc, model):\n",
    "    \"\"\"\n",
    "    Convert a document to vectors (average and sum) based on its word embeddings.\n",
    "    \n",
    "    Args:\n",
    "        doc (str): Preprocessed document.\n",
    "        model (gensim model): Pre-trained word embeddings model.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (average_vector, sum_vector)\n",
    "    \"\"\"\n",
    "    words = doc.split()\n",
    "    word_vectors = [model[word] for word in words if word in model]\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(model.vector_size), np.zeros(model.vector_size)\n",
    "    word_vectors = np.array(word_vectors)\n",
    "    return np.mean(word_vectors, axis=0), np.sum(word_vectors, axis=0)\n",
    "\n",
    "def compute_text_representations(documents):\n",
    "    \"\"\"\n",
    "    Compute TF, TF-IDF, and Word2Vec embeddings (average and sum) for a list of documents.\n",
    "    \n",
    "    Args:\n",
    "        documents (list of str): List of raw text documents.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary with 'tf', 'tfidf', 'word2vec_avg', and 'word2vec_sum' representations.\n",
    "    \"\"\"\n",
    "    # Preprocess documents\n",
    "    preprocessed_docs = [preprocess_text(doc) for doc in documents]\n",
    "    \n",
    "    # TF and TF-IDF\n",
    "    tf_vectorizer = TfidfVectorizer(use_idf=False, norm=None)\n",
    "    tf_matrix = tf_vectorizer.fit_transform(preprocessed_docs)\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_docs)\n",
    "    \n",
    "    # Word2Vec embeddings (average and sum)\n",
    "    word2vec_avg_vectors = []\n",
    "    word2vec_sum_vectors = []\n",
    "    for doc in preprocessed_docs:\n",
    "        avg_vector, sum_vector = document_to_vectors(doc, word2vec_model)\n",
    "        word2vec_avg_vectors.append(avg_vector)\n",
    "        word2vec_sum_vectors.append(sum_vector)\n",
    "    \n",
    "    word2vec_avg_vectors = np.array(word2vec_avg_vectors)\n",
    "    word2vec_sum_vectors = np.array(word2vec_sum_vectors)\n",
    "    \n",
    "    return {\n",
    "        \"tf\": tf_matrix,\n",
    "        \"tfidf\": tfidf_matrix,\n",
    "        \"word2vec_avg\": word2vec_avg_vectors,\n",
    "        \"word2vec_sum\": word2vec_sum_vectors,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "representations = compute_text_representations(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_representations(representations, file_prefix):\n",
    "    \"\"\"\n",
    "    Save text representations (TF, TF-IDF, Word2Vec Avg, Word2Vec Sum) to disk.\n",
    "\n",
    "    Args:\n",
    "        representations (dict): Dictionary containing the representations.\n",
    "        file_prefix (str): Prefix for the output files.\n",
    "    \"\"\"\n",
    "    # Save TF and TF-IDF as dense arrays\n",
    "    np.save(file_prefix + \"_tf.npy\", representations[\"tf\"].toarray())\n",
    "    np.save(file_prefix + \"_tfidf.npy\", representations[\"tfidf\"].toarray())\n",
    "    \n",
    "    # Save Word2Vec representations\n",
    "    np.save(file_prefix + \"_word2vec_avg.npy\", representations[\"word2vec_avg\"])\n",
    "    np.save(file_prefix + \"_word2vec_sum.npy\", representations[\"word2vec_sum\"])\n",
    "    \n",
    "    # Optionally save as a single pickle file\n",
    "    with open(file_prefix + \".pkl\", \"wb\") as f:\n",
    "        pickle.dump(representations, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_representations(representations, \"02_text_representations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
